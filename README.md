# üîç Recherche Approfondie SEO - Analyse SERP Centric 

![version](https://img.shields.io/badge/version-2.0-blue)
![license](https://img.shields.io/badge/license-MIT-green)
![python](https://img.shields.io/badge/python-3.10%2B-blue)
[![Open in GitHub Codespaces](https://img.shields.io/badge/GitHub_Codespaces-Open-blue?logo=github)](https://codespaces.new/Mathieu-IA/readctor_seo)

**L'outil professionnel d'analyse SERP et g√©n√©ration de contenu SEO pilot√© par IA**  
*Int√®gre donn√©es concurrentielles et recherches approfondies pour des recommandations data-driven*

[![Vid√©o d√©mo](https://img.shields.io/badge/üì∫-Voir_la_d√©mo-FF0000?style=flat)](https://example.com/demo.mp4)
[![Discord](https://img.shields.io/badge/üí¨-Rejoindre_le_Discord-5865F2?logo=discord)](https://discord.gg/example)

## üåü Fonctionnalit√©s Principales

| Cat√©gorie         | Fonctionnalit√©s                                                                 |
|-------------------|---------------------------------------------------------------------------------|
| üîé Analyse SERP   | Scraping complet ‚Ä¢ Extraction H2/H3 
‚Ä¢ Benchmark meta-titles 
‚Ä¢ Cartographie s√©mantique |
| üß† Intelligence    | G√©n√©ration de plans ‚Ä¢ Optimisation LLM ‚Ä¢ D√©tection d'opportunit√©s ‚Ä¢ Alertes SEO |
| üìà Reporting       | Export PDF/HTML ‚Ä¢ Dashboard interactif ‚Ä¢ Comparatif historique ‚Ä¢ Stats temps r√©el |
| ‚öôÔ∏è Infrastructure  | Proxy rotatif ‚Ä¢ Cache intelligent ‚Ä¢ Architecture modulaire ‚Ä¢ API REST          |

## üöÄ Quick Start

```bash
# Installation
git clone https://github.com/Mathieu-IA/readctor_seo
cd readctor_seo
pip install -r requirements.txt

# Configuration
cp .env.example .env
nano .env  # Ajoutez vos cl√©s API

# Lancement
streamlit run app.py
```

## üì¶ Stack Technique

```mermaid
pie
    title R√©partition des Technologies
    "Python" : 45
    "Streamlit (UI)" : 20
    "LLM (GPT-4/Gemini)" : 15
    "Syst√®me de Cache" : 10
    "Autres (Jinja2, Pydantic)" : 10
```

- Architecture modulaire et extensible
- Int√©gration multi-fournisseurs de LLM
- Templates de prompts dynamiques (Jinja2)
- Rotation automatique de proxies
- Validation stricte des donn√©es (Pydantic)

## Architecture Technique

```mermaid
graph TD
    A[Interface Streamlit] --> B[SERP Scraper]
    A --> C[Research Engine]
    B --> D{Analyse SERP}
    C --> E{Recherche Perplexity}
    D --> F[Content Engine]
    E --> F
    F --> G[Plan SEO Structur√©]
    G --> H[Article Generator]
```

## Architecture Compl√®te

### Composants Principaux

**Core** :
- `article_plan.py` : Mod√®le Pydantic pour les plans SEO avec versioning
- `serp_result.py` : Analyse des positions/performances SERP
- `keyword.py` : Gestion TF-IDF et clustering s√©mantique
- `project.py` : Configuration des projets utilisateur
- `generated_article.py` : Rendering Markdown/HTML des articles

**Modules** :
- `data_manager.py` : Persistance des donn√©es (JSON/Parquet)
- `llm_service.py` : Abstraction multi-fournisseur (OpenAI/Anthropic/Gemini)
- `markdown_utils.py` : G√©n√©ration de rapports Markdown
- `batch_processor.py` : Traitement asynchrone par lots
- `web_scraper.py` : Extraction de contenu web avanc√©
- `api_response_handler.py` : Normalisation des r√©ponses externes

### Composants Cl√©s

- R√©cup√©ration des donn√©es SERP brutes
- Analyse des meta-titles/descriptions concurrents
- Extraction des structures H2/H3
- Scraping du contenu des Top 5 r√©sultats

### ResearchEngine (modules/research_engine.py)
- G√©n√©ration dynamique de requ√™tes de recherche
- Int√©gration avec Perplexity API
- Validation et structuration des r√©sultats
- Conservation des sources et dates

### ContentEngine (modules/content_engine.py)
- Fusion des donn√©es SERP et recherches
- G√©n√©ration du plan via LLM
- Pr√©servation des donn√©es brutes
- Structure JSON normalis√©e

## Flux de Donn√©es

1. **Entr√©e** 
   - Mot-cl√© principal 
   - Param√®tres de recherche
   - Mod√®le LLM s√©lectionn√©

2. **Traitement**
   ```mermaid
   sequenceDiagram
       Utilisateur->>+SERPScraper: Lance l'analyse SERP
       SERPScraper->>+ResearchEngine: G√©n√®re les requ√™tes
       ResearchEngine->>+Perplexity: Ex√©cute les recherches
       Perplexity-->>-ResearchEngine: R√©sultats bruts
       ResearchEngine->>+ContentEngine: Structure les donn√©es
       ContentEngine->>+LLM: G√©n√®re le plan
       LLM-->>-ContentEngine: Plan structur√©
       ContentEngine->>+UI: Affiche le r√©sultat
   ```

3. **Sortie**
   - Plan SEO complet avec :
     - `serp_data`: Donn√©es brutes de la SERP
     - `research_data`: R√©sultats des recherches approfondies
     - `competitor_analysis`: Contenu scrapp√© des concurrents

## Structure Technique du Plan

```json
{
  "keyword": "exemple",
  "serp_analysis": {
    "top_competitors": [
      {
        "position": 1,
        "meta_title": "...",
        "h1": "...",
        "h2_structure": ["...", "..."],
        "content_snippet": "..."
      }
    ],
    "average_content_length": 2450,
    "common_keywords": ["...", "..."]
  },
  "research_insights": {
    "latest_stats": [
      {
        "value": "72%",
        "source": "Etude SEMrush 2024",
        "date": "2024-03-15"
      }
    ],
    "trends": [
      {
        "trend_name": "...",
        "growth_rate": "+18%/an",
        "related_queries": ["...", "..."]
      }
    ]
  },
  "content_strategy": {
    "recommended_structure": {
      "h1": "...",
      "h2_sections": [
        {
          "title": "...",
          "target_keywords": ["...", "..."],
          "competitor_references": [1, 3]
        }
      ]
    },
    "optimization_checklist": [
      "Balise meta optimis√©e pour le Top 3 SERP",
      "Int√©gration des statistiques r√©centes",
      "R√©ponse aux questions top 5 des forums"
    ]
  }
}
```

## Proc√©dure de Test

1. Lancer l'application avec monitoring :
   ```bash
   streamlit run app.py --server.port 8502 --logger.level debug
   ```
   
2. Ex√©cuter les tests unitaires :
   ```bash
   pytest tests/ -v -m "serp or research"
   ```

3. Dans l'interface :
   - Cr√©er un nouveau projet
   - Entrer un mot-cl√© cible
   - Activer "Analyse SERP" et "Recherche approfondie"

4. Apr√®s g√©n√©ration :
   - V√©rifier dans la console :
     ```javascript
     console.log('Plan complet:', planData);
     ```
   - Valider la pr√©sence des sections :
     - `serp_analysis.top_competitors`
     - `research_insights.latest_stats`
     - `content_strategy.optimization_checklist`

5. Exporter les donn√©es :
   ```python
   # Exemple d'acc√®s aux donn√©es
   print(plan['serp_analysis']['average_content_length'])
   print(plan['research_insights']['trends'][0]['growth_rate'])
   ```

## Modifications R√©centes

### app.py
- **Configuration proxy avanc√©e** :
  ```python
  # config.py
  SERP_PROXY = {
      "host": os.getenv("PROXY_HOST"),
      "port": os.getenv("PROXY_PORT"),
      "auth": (os.getenv("PROXY_USER"), os.getenv("PROXY_PASS")),
      "rotation_interval": 300  # Rotation toutes les 5 minutes
  }
  
  BRIGHTDATA_API_TOKEN = os.getenv("BRIGHTDATA_API_TOKEN")
  BRIGHTDATA_SERP_ZONE_NAME = os.getenv("BRIGHTDATA_SERP_ZONE_NAME", "serp")
  ```
- **Gestion asynchrone** via `asyncio` et `aiohttp`
- **Stockage session** dans des fichiers JSON temporaires (`/sessions`)
- **Syst√®me de templates** Jinja2 avec validation de sch√©ma :
  ```python
  def load_prompt_template(name):
      with open(f"templates/{name}.j2") as f:
          template = Template(f.read())
      return template
  ```
- **Gestion d'erreurs** avec reprise automatique des requ√™tes

### research_engine.py
- Validation des r√©sultats de recherche
- Normalisation des formats de dates
- D√©tection automatique de contradictions
- Archivage JSON des r√©sultats bruts

### content_engine.py
- **Syst√®me de scoring** bas√© sur 12 m√©triques SERP :
  ```python
  def calculate_score(competitor):
      return (competitor['position'] * 0.3 
              + competitor['content_length'] * 0.2 
              + competitor['keyword_density'] * 0.5)
  ```
- **Templates dynamiques** avec h√©ritage Jinja2 :
  ```jinja
  {% extends 'base_prompt.j2' %}
  {% block analysis %}...{% endblock %}
  ```
- **Checklists interactives** g√©n√©r√©es via AST parser
- **D√©tection d'opportunit√©s** par analyse TF-IDF
- **Validation JSON** via sch√©ma Pydantic strict

## Sch√©ma d'Int√©gration des Donn√©es

```mermaid
graph LR
    A[SERP] --> B((Analyse))
    B --> C{Structure}
    C --> D[Plan]
    D --> E{Validation}
    E --> F[Article]
    
    G[Recherche] --> B
    H[LLM] --> C
    
    style A fill:#f9f,stroke:#333
    style G fill:#9f9,stroke:#333

    %% L√©gende
    classDef serp fill:#f9f,stroke:#333;
    classDef research fill:#9f9,stroke:#333;
    classDef process fill:#99f,stroke:#333;
    
    legend
        Donn√©es SERP:::serp |
        Recherches:::research |
        Processus:::process
    end

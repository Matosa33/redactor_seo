# üöÄ SEO Content Generator Pro - R√©dacteur SEO IA

![version](https://img.shields.io/badge/version-1.0-blue) 
![license](https://img.shields.io/badge/license--Assuming_MIT-green) 
![python](https://img.shields.io/badge/python-3.10%2B-blue)
[![Open in GitHub Codespaces](https://img.shields.io/badge/GitHub_Codespaces-Open-blue?logo=github)](https://codespaces.new/Mathieu-IA/readctor_seo) 

**L'outil Streamlit pour l'analyse SERP, la recherche IA et la g√©n√©ration de contenu SEO (Plans & Articles)**
*G√©n√©rez des plans d'articles structur√©s et des r√©dactions compl√®tes bas√©s sur les donn√©es SERP, le contenu des concurrents et des recherches IA optionnelles.*

## üåü Fonctionnalit√©s R√©ellement Impl√©ment√©es

| Cat√©gorie                | Fonctionnalit√©s                                                                                                                                 |
| :----------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------- |
| üóÇÔ∏è **Gestion Projet** | ‚Ä¢ Organisation par Projets et Mots-Cl√©s.<br> ‚Ä¢ Sauvegarde/Chargement des plans (JSON) et articles (Markdown).                                      |
| üîç **Analyse SERP** | ‚Ä¢ R√©cup√©ration des r√©sultats Google SERP via Bright Data (Proxy `brd_json=1`).<br> ‚Ä¢ Mise en cache des r√©sultats SERP.<br> ‚Ä¢ Scraping du contenu texte des Top URLs de la SERP. |
| üß† **Recherche IA** | ‚Ä¢ G√©n√©ration dynamique de requ√™tes de recherche bas√©es sur le mot-cl√©/SERP.<br> ‚Ä¢ Int√©gration Perplexity API (si cl√© fournie) ou Simulation LLM.<br> ‚Ä¢ Extraction d'insights structur√©s √† partir des r√©sultats de recherche. |
| ‚úçÔ∏è **G√©n√©ration Contenu** | ‚Ä¢ G√©n√©ration de Plans d'articles structur√©s (JSON) via LLM, int√©grant SERP/Recherche.<br> ‚Ä¢ R√©daction d'Articles (Markdown) bas√©e sur le plan et la recherche.<br> ‚Ä¢ Support multi-LLM (OpenAI, Anthropic, Google, DeepSeek). |
| ‚ú® **Fonctions Add.** | ‚Ä¢ Traitement par Lots Asynchrone (Plans/Articles) avec suivi de progression.<br> ‚Ä¢ G√©n√©ration d'images d'illustration via Google Gemini.<br> ‚Ä¢ Export des plans/articles (JSON, MD, HTML, DOCX, WordPress HTML). |
| üñ•Ô∏è **Interface & Infra.** | ‚Ä¢ Interface utilisateur bas√©e sur Streamlit avec onglets.<br> ‚Ä¢ Configuration via fichier `.env`.<br> ‚Ä¢ Architecture modulaire (Core / Modules).                               |

## üöÄ D√©marrage Rapide

# 1. Cloner le d√©p√¥t (si disponible)
```bash
# git clone [https://github.com/VOTRE_USER/readctor_seo](https://github.com/VOTRE_USER/readctor_seo) 
# cd readctor_seo
```

# 2. Installer les d√©pendances
pip install -r requirements.txt

# 3. Configurer les cl√©s API
cp .env.example .env  # Cr√©ez votre fichier .env s'il n'existe pas
nano .env          # Ajoutez vos cl√©s API (OpenAI, Google, BrightData, etc.)

# 4. Lancer l'application Streamlit
streamlit run app.py

## üì¶ Stack Technique Principale

```mermaid
pie
    title R√©partition Approximative des Technologies (Code Principal)
    "Python (Logique M√©tier)" : 40
    "Streamlit (Interface UI)" : 25
    "Biblioth√®ques LLM (openai, google, etc.)" : 15
    "Scraping/Requ√™tes (httpx, aiohttp, bs4)" : 10
    "Gestion Donn√©es (json, markdown)" : 5
    "Autres (dotenv, asyncio)" : 5
```

- **Langage:** Python 3.10+
- **Interface Utilisateur:** Streamlit
- **LLMs:** Int√©gration via biblioth√®ques `openai`, `anthropic`, `google-generativeai` (+ `requests`/`httpx` pour DeepSeek/Perplexity)
- **Appels Asynchrones:** `asyncio`, `httpx`, `aiohttp`
- **Scraping SERP:** `aiohttp` (pour Bright Data via proxy), `beautifulsoup4` & `lxml` (pour contenu concurrent)
- **Gestion Donn√©es:** Modules `json`, `pathlib`, `shutil`. Stockage simple (JSON pour plans/metadata, MD pour articles).
- **Gestion D√©pendances:** `pip` et `requirements.txt`
- **Configuration:** `python-dotenv`

## Architecture Simplifi√©e

```mermaid
graph TD
    subgraph "Interface Utilisateur (app.py)"
        A[UI Streamlit]
    end

    subgraph "Logique M√©tier (Modules)"
        B[DataManager]
        C[LLMService]
        D[ContentEngine]
        E[SERPScraper]
        F[ResearchEngine]
        G[WebScraper]
        H[BatchProcessor]
        I[MarkdownUtils]
    end

    subgraph "Coeur Applicatif (Core)"
        J[Project]
        K[Keyword]
        L[ArticlePlan]
        M[GeneratedArticle]
        N[SERPResult]
    end

    subgraph "Services Externes"
        O[API LLMs]
        P[Bright Data / Google SERP]
        Q[Perplexity API]
        R[Sites Web Concurrents]
    end

    subgraph "Syst√®me de Fichiers"
        S[Cache SERP]
        T[Dossiers Projet/Mot-Cl√©]
    end

    A -->|G√®re Projets/Mots-cl√©s| B
    A -->|D√©clenche G√©n√©ration| D
    A -->|D√©clenche Traitement Lots| H
    A -->|Utilise Utilitaires MD| I

    D -->|Utilise LLM| C
    D -->|Appelle Analyse SERP| E
    D -->|Appelle Recherche| F
    D -->|Sauvegarde/Charge via| B

    F -->|Utilise LLM| C
    F -->|Appelle API Perplexity| Q

    E -->|R√©cup√®re SERP via| P
    E -->|Lit/√âcrit Cache| S

    G -->|Scrape Sites| R

    H -->|Ex√©cute T√¢ches Plan ou Article| D  // Ligne Corrig√©e

    B -->|Lit/√âcrit Projets| T

    C -->|Appelle API LLMs| O

    B -.-> J & K
    D -.-> L & M & N
    E -.-> N

    style S fill:#f9f,stroke:#333
    style T fill:#f9f,stroke:#333
```

## Description des Composants

**Core (`core/`)**:
- `project.py`: Repr√©sente un projet SEO, g√®re les m√©tadonn√©es du projet.
- `keyword.py`: Repr√©sente un mot-cl√© sp√©cifique dans un projet, g√®re les chemins vers le plan et l'article.
- `article_plan.py`: Repr√©sente le plan d'article structur√© (charg√©/sauv√© en JSON).
- `generated_article.py`: Repr√©sente le contenu de l'article g√©n√©r√© (charg√©/sauv√© en Markdown).
- `serp_result.py`: Repr√©sente les donn√©es SERP mises en cache pour une requ√™te.

**Modules (`modules/`)**:
- `data_manager.py`: G√®re le chargement et la sauvegarde des projets, mots-cl√©s, plans et articles sur le syst√®me de fichiers.
- `llm_service.py`: Fournit une interface unifi√©e pour interagir avec diff√©rents fournisseurs LLM (OpenAI, Anthropic, Google, DeepSeek) et pour la g√©n√©ration d'images (Google). G√®re les cl√©s API.
- `serp_scraper.py`: R√©cup√®re les r√©sultats de recherche Google en utilisant l'API Bright Data (via proxy `brd_json=1`) et g√®re la mise en cache.
- `web_scraper.py`: Scrape et extrait le contenu textuel principal des URLs fournies (concurrents SERP).
- `research_engine.py`: Orchestre la recherche approfondie en g√©n√©rant dynamiquement des requ√™tes, en interrogeant Perplexity API (ou simulation LLM) et en structurant les r√©sultats.
- `content_engine.py`: G√©n√®re les plans d'articles (JSON) et les articles (Markdown) en utilisant `LLMService` et en int√©grant les donn√©es SERP, le contenu concurrent et la recherche.
- `batch_processor.py`: G√®re l'ex√©cution asynchrone de t√¢ches (g√©n√©ration de plans/articles) pour plusieurs mots-cl√©s, avec gestion de la concurrence, progression, pause/reprise/arr√™t.
- `markdown_utils.py`: Fonctions utilitaires pour convertir le Markdown en HTML, DOCX, et format compatible WordPress. Convertit √©galement les plans JSON en Markdown pour affichage.
- `api_response_handler.py`: (Utilisation limit√©e observ√©e) Vise √† standardiser les r√©ponses des API.
- `config.py`: Charge et fournit les param√®tres de configuration depuis les variables d'environnement / fichier `.env`.

## Flux Principal (G√©n√©ration Plan -> Article)

1.  **Entr√©e Utilisateur (via `app.py`)**:
    * S√©lection/Cr√©ation d'un `Project`.
    * Entr√©e/S√©lection d'un `Keyword`.
    * Choix des options (Analyse SERP, Recherche Approfondie).
    * S√©lection des mod√®les LLM.

2.  **Analyse SERP (si activ√©e)**:
    * `app.py` appelle `SERPScraper.get_serp_results(keyword)`.
    * `SERPScraper` v√©rifie le cache (`SERPResult`). Si absent/expir√©, appelle Bright Data via proxy (`aiohttp`), r√©cup√®re les r√©sultats JSON (`brd_json=1`).
    * Standardise les r√©sultats organiques (`rank`, `url`, `meta_title`, `meta_description`, `domain`).
    * Sauvegarde dans le cache (`SERPResult.save()`).
    * Retourne les r√©sultats standardis√©s √† `app.py`.

3.  **Scraping Concurrents (si SERP r√©ussie)**:
    * `app.py` extrait les URLs du Top N des r√©sultats SERP.
    * `app.py` appelle `WebScraper.scrape_urls_content(urls)`.
    * `WebScraper` r√©cup√®re le contenu HTML (`httpx`), extrait le texte principal (`BeautifulSoup4`), et retourne un dictionnaire `url: contenu`.

4.  **Recherche Approfondie (si activ√©e)**:
    * `app.py` g√©n√®re des requ√™tes de recherche dynamiques via `LLMService` bas√© sur mot-cl√©/SERP/concurrents.
    * `app.py` appelle `ResearchEngine.perform_research(queries, model_settings)`.
    * `ResearchEngine` appelle Perplexity API (`openai` SDK si configur√©) ou simule avec `LLMService`.
    * Extrait les insights via `LLMService`.
    * Retourne une liste d'objets `ResearchResult` √† `app.py`.

5.  **G√©n√©ration du Plan**:
    * `app.py` appelle `ContentEngine.generate_plan(...)` avec le mot-cl√©, les r√©sum√©s SERP/concurrents/recherche, et les param√®tres LLM.
    * `ContentEngine` formate un prompt sp√©cifique et appelle `LLMService.generate_content()`.
    * `ContentEngine` parse la r√©ponse JSON du LLM et cr√©e un objet `ArticlePlan`.
    * `app.py` sauvegarde le plan via `DataManager.save_plan()`.

6.  **G√©n√©ration de l'Article**:
    * L'utilisateur navigue vers l'onglet R√©dacteur.
    * `app.py` charge le `ArticlePlan` via `DataManager.load_plan()`.
    * L'utilisateur configure les param√®tres de r√©daction (ton, longueur) et le mod√®le LLM.
    * `app.py` appelle `ContentEngine.generate_article(...)` avec l'objet `ArticlePlan`, le r√©sum√© de la recherche (si dispo), et les param√®tres LLM.
    * `ContentEngine` formate le prompt et appelle `LLMService.generate_content()`.
    * `ContentEngine` cr√©e un objet `GeneratedArticle`.
    * `app.py` sauvegarde l'article via `DataManager.save_article()`.

7.  **Sortie/Affichage (via `app.py`)**:
    * Affichage du plan (Markdown/JSON) et de l'article (Markdown/HTML).
    * Options d'√©dition et d'export (MD, HTML, DOCX, WP).

```mermaid
   sequenceDiagram
       participant UI as Interface Streamlit (app.py)
       participant DM as DataManager
       participant SS as SERPScraper
       participant WS as WebScraper
       participant RE as ResearchEngine
       participant CE as ContentEngine
       participant LLM as LLMService
       participant Cache
       participant FS as FileSystem (Projets)
       participant ExtAPI as APIs Externes (BrightData, LLM, Perplexity)

       UI->>+DM: Charger/Cr√©er Projet
       DM->>FS: Lire/√âcrire metadata.json
       DM-->>-UI: Projet Charg√©

       UI->>+SS: get_serp_results(keyword)
       SS->>Cache: V√©rifier Cache
       alt Cache Miss ou Expir√©
           SS->>ExtAPI: Appel Bright Data (Proxy)
           ExtAPI-->>SS: R√©sultats JSON SERP
           SS->>Cache: Sauver R√©sultat
       else Cache Hit
           Cache-->>SS: R√©sultat mis en cache
       end
       SS-->>-UI: R√©sultats SERP (liste)

       opt Scraping Concurrents
           UI->>+WS: scrape_urls_content(urls_serp)
           WS->>ExtAPI: Requetes HTTP concurrents
           ExtAPI-->>WS: Contenu HTML
           WS-->>-UI: Contenu scrapp√© (dict)
       end

       opt Recherche Approfondie
           UI->>+LLM: G√©n√©rer requ√™tes recherche(prompt_context)
           LLM->>ExtAPI: Appel LLM (mod√®le rapide)
           ExtAPI-->>LLM: R√©ponse (JSON de requ√™tes)
           LLM-->>-UI: Liste de requ√™tes
           UI->>+RE: perform_research(queries)
           RE->>ExtAPI: Appels Perplexity ou LLM (Simulation)
           ExtAPI-->>RE: R√©sultats recherche bruts
           RE->>+LLM: Extraire Insights(r√©sultats_bruts)
           LLM->>ExtAPI: Appel LLM (mod√®le extraction)
           ExtAPI-->>LLM: R√©ponse (JSON d'insights)
           LLM-->>-RE: Insights structur√©s
           RE-->>-UI: ResearchResults (liste)
       end

       UI->>+CE: generate_plan(keyword, serp_summary, research_summary)
       CE->>+LLM: generate_content(prompt_plan)
       LLM->>ExtAPI: Appel LLM (mod√®le plan)
       ExtAPI-->>LLM: R√©ponse (JSON du plan)
       LLM-->>-CE: Contenu JSON Plan
       CE-->>-UI: ArticlePlan (objet)
       UI->>+DM: save_plan(keyword, plan_obj)
       DM->>FS: √âcrire plan.json
       DM-->>-UI: Confirmation Sauvegarde

       UI->>+CE: generate_article(plan_obj, research_summary)
       CE->>+LLM: generate_content(prompt_article)
       LLM->>ExtAPI: Appel LLM (mod√®le article)
       ExtAPI-->>LLM: R√©ponse (Markdown article)
       LLM-->>-CE: Contenu Markdown Article
       CE-->>-UI: GeneratedArticle (objet)
       UI->>+DM: save_article(keyword, article_obj)
       DM->>FS: √âcrire article.md
       DM-->>-UI: Confirmation Sauvegarde
```

## Structure du Plan G√©n√©r√© (JSON)

Bas√© sur l'analyse des prompts et des exemples de plans (`coder_avec_Cline/plan.json`, etc.):

```json
{
  "keyword": "string - Le mot-cl√© cibl√©",
  "metatitle": "string - Suggestion de m√©ta-titre optimis√©",
  "metadescription": "string - Suggestion de m√©ta-description engageante",
  "h1": "string - Suggestion de titre H1 principal",
  "backstory": "string - Contexte/objectif de l'article bas√© sur l'analyse",
  "data": {
    "secondary_keywords": ["string"], // Liste de mots-cl√©s secondaires pertinents
    "key_insights": ["string"]      // Liste des points/faits cl√©s issus de l'analyse/recherche
  },
  "plan": [ // Liste des sections principales de l'article
    {
      "h2": "string - Titre de la section H2",
      "h3": ["string"], // Liste optionnelle de sous-titres H3
      "brief": "string - Instructions/points cl√©s √† couvrir dans cette section, bas√©s sur l'analyse"
    }
    // ... autres sections H2 ...
  ]
}
```

## Proc√©dure de Test (Bas√©e sur l'UI)

1.  **Lancement:**
    * Assurez-vous que les cl√©s API sont dans `.env`.
    * Lancez `streamlit run app.py`.

2.  **Gestion Projet:**
    * Cr√©ez un nouveau projet.
    * S√©lectionnez le projet cr√©√©.

3.  **G√©n√©ration Plan:**
    * Allez √† l'onglet "G√©n√©rateur de Plan".
    * Entrez un mot-cl√© (ex: "marketing de contenu IA").
    * Cochez "Analyse SERP Google".
    * Cochez "Activer la recherche approfondie" (si Perplexity ou LLM configur√©).
    * S√©lectionnez un mod√®le LLM pour le plan.
    * Cliquez sur "üöÄ G√©n√©rer le Plan".
    * **V√©rification:**
        * Attendez la fin du spinner.
        * V√©rifiez les messages de succ√®s/erreur pour SERP et Recherche.
        * Le plan JSON doit s'afficher dans l'√©diteur.
        * Les onglets "SERP" et "Recherche" doivent contenir des donn√©es (si activ√©).
        * V√©rifiez que le fichier `plan.json` a √©t√© cr√©√© dans `projects/NOM_PROJET/keywords/NOM_MOTCLE/`.

4.  **G√©n√©ration Article:**
    * Allez √† l'onglet "R√©dacteur d'Articles".
    * V√©rifiez que le plan pr√©c√©dent est charg√© (infos dans l'expander).
    * S√©lectionnez un mod√®le LLM pour l'article.
    * Cliquez sur "‚úçÔ∏è R√©diger l'Article".
    * **V√©rification:**
        * Attendez la fin du spinner.
        * L'article Markdown doit s'afficher dans l'√©diteur.
        * V√©rifiez que le fichier `article.md` a √©t√© cr√©√© dans le dossier du mot-cl√©.

5.  **G√©n√©ration Image (si Google API configur√©):**
    * Allez √† l'onglet "Raffineur de Contenu".
    * Allez au sous-onglet "√âl√©ments Visuels".
    * Cliquez sur "G√©n√©rer Illustrations".
    * **V√©rification:**
        * Attendez la fin du spinner.
        * Des images et leur code Markdown doivent s'afficher.

6.  **Traitement par Lots:**
    * Allez √† l'onglet "Traitement par Lots".
    * Entrez plusieurs mots-cl√©s manuellement.
    * S√©lectionnez "G√©n√©rer Plan" et/ou "G√©n√©rer Article".
    * Configurez les mod√®les LLM pour le batch.
    * Cliquez sur "üöÄ D√©marrer le Traitement".
    * **V√©rification:**
        * La barre de progression doit s'afficher.
        * V√©rifiez les logs et les r√©sultats d√©taill√©s dans l'expander.
        * V√©rifiez la cr√©ation des fichiers `plan.json`/`article.md` pour les mots-cl√©s trait√©s.
        * Testez les boutons Pause/Reprendre/Arr√™ter.

7.  **Exports:**
    * Testez les diff√©rents boutons d'export (JSON, MD, HTML, DOCX, WP) pour le plan et l'article.

```

Ce README est g√©n√©r√© en analysant la structure du code, les imports, les classes, les fonctions et les interactions observ√©es dans `app.py` et les diff√©rents modules. Il √©vite d'inclure des fonctionnalit√©s mentionn√©es dans votre exemple mais non trouv√©es dans le code fourni (comme le scoring SERP, TF-IDF, Parquet, Pydantic, Jinja2, etc.).
